{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharma7056/renuinde577project/blob/main/SupervisedLearning/5%20-%20Neural%20Network/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z851P51DxbAE"
      },
      "source": [
        "# The Multilayer Perceptron Learning Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfyVYdk3xbAJ"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5dlv93lxbAK"
      },
      "source": [
        "#### The goal of learning\n",
        "MLP algorithm has four elements:\n",
        "\n",
        "* Input features\n",
        "* Weights and bias\n",
        "* Weighted sum (net sum)\n",
        "* Activation function\n",
        "\n",
        "The objective is to identify weights and biases that can best predict the output true label/value.\n",
        "\n",
        "#### The Activation Function\n",
        "\n",
        "The activation function is used to convert perceptron output. There are two activation functions often used in MLP, sigmoid and ReLU.\n",
        "\n",
        "This notebook uses the **sigmoid** function.\n",
        "\n",
        "$$\\sigma(z)=\\frac {1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "Define $L$ is the number of layers, for $l=1,...,L-1$, the preactivation phase is\n",
        "$$z^l=w^l a^{l-1}+b^l$$\n",
        "\n",
        "The postactivation phase is\n",
        "$$a^l=\\sigma(z^l)$$\n",
        "and $a^0=x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyz1VaJLxbAL"
      },
      "source": [
        "Another commonly used activation function is **ReLU**, Rectified Linear Units.\n",
        "\n",
        "$$R(z)= max(0,z)$$\n",
        "\n",
        "The function returns $0$ when the input is negative, and returns the input value when the input is positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evzhOaIwxbAL"
      },
      "source": [
        "\n",
        "\n",
        "#### The Loss Function\n",
        "\n",
        "The loss function is a function represents the differences between the actual value and the predicted value. Loss function provides useful information to update the weights and biases, to minimize the differences, or loss. The total loss of the model would be the sum of the loss on each input data.\n",
        "\n",
        "In our algorithm, we use the Mean Square Error:\n",
        "\n",
        "$$C(w,b;x,y)=\\frac{1}{2} \\sum_{i=1}^n(a_i^{l-1}-y_i)^2$$\n",
        "\n",
        "#### Output Error\n",
        "\n",
        "The output error is\n",
        "\n",
        "$$\\delta^{l-1}=\\triangledown_{a^{l-1}}C \\otimes \\sigma'(z^{l-1})$$\n",
        "\n",
        "#### Neuron Error\n",
        "\n",
        "According to the output error, for $l=L-2,...,1$, the neuron error is\n",
        "\n",
        "$$\\delta^{l}=\\left ( (w^{l+1})^T a^{l+1} \\right ) \\otimes \\sigma'(z^{l})$$\n",
        "\n",
        "#### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "To optimize the parameters, again, we use the idea of gradient descent algorithm. In practice, usually a modification of the gradient descent is used, especially when the dataset is big. In Stochastic Gradient Descent, in each iteration, one data point from the whole dataset is selected randomly and calculated the gradient. Because of the randomness in the SGD, it usually takes a higher number of iterations to reach the minimum.\n",
        "\n",
        "In each epoch of SGD, the working flow is: 1) take an example; 2) feed to the neural network; 3) calculate the gradient and update the weights; 4) repeat 1-3 until the loss converges or reach the maximum number of iteration.\n",
        "\n",
        "Combine with our algorithm, for $l=1,...,L-1$, to update the weights,\n",
        "\n",
        "$$w^l = w^l - \\alpha \\delta^l (a^{l-1})^T$$\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "#### Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-batch gradient descent is the most common implementation of gradient descent in deep learning or neural network. Instead of using the whole dataset for computation, in mini-batch gradient descent, the training dataset is split into small batches (mini-batches), and these mini-batches are used to calculate and update the gradient. Mini-batch gradient descent can greatly speed up the algorithm.\n",
        "\n",
        "In one epoch of the mini-batch gradient descent, the working flow is: 1) pick a mini-batch; 2) feed to the neural network; 3) calculate the mean gradient and update the weights; 4) repeat 1-3 for the generated mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Oxo1H-xbAM"
      },
      "source": [
        "---\n",
        "\n",
        "## Implementation\n",
        "\n",
        "This notebook implements MLP with a single input layer with $784$ input nodes, 2 hidden layers of arbitrary size ($60$ nodes per layer), and $10$ output nodes. These layers will be denoted $L^0, L^1, L^2,$ and $L^{3}$, respectively.\n",
        "\n",
        "I use sigmoid function and Mean Squared Error as the activation function and loss function respectively.\n",
        "\n",
        "For $l = 1, 2, 3$, layer $l$ will have two phases:\n",
        "\n",
        "* The preactivation phase $z^l = W^la^{l-1} + b^l,$\n",
        "* The postactivation phase $a^l = \\sigma(z^l).$\n",
        "\n",
        "The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through a chosen activation function element wise. For notational convenience, we let $a^0 = x$, where $x$ is the current input data into our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDAVp4yDxbAN"
      },
      "source": [
        "### About the Fashion MNIST Dataset\n",
        "\n",
        "The [Fashion MNIST](https://keras.io/api/datasets/fashion_mnist/) dataset ia an alternative dataset to the MNIST. It consists of $70000$ $28\\times 28$ grayscale images of $10$ fashion categories, $60000$ of which are typically used as labeled training examples, while the other $10000$ are used for testing your learning model on. The following picture represent a sample of some of the images.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/sharma7056/renuinde577project/blob/main/SupervisedLearning/5%20-%20Neural%20Network/Image/fashion_mnist-3.0.1.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "Like the images above, each training and test example is assigned to one of the following labels:\n",
        "\n",
        "* 0 T-shirt/top\n",
        "* 1 Trouser\n",
        "* 2 Pullover\n",
        "* 3 Dress\n",
        "* 4 Coat\n",
        "* 5 Sandal\n",
        "* 6 Shirt\n",
        "* 7 Sneaker\n",
        "* 8 Bag\n",
        "* 9 Ankle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gRQ3J2xbAP"
      },
      "source": [
        "### Task\n",
        "\n",
        "To classify fashion categories using the Multilayer Perceptron Learning algorithm based on the Fashion MNIST dataset.\n",
        "\n",
        "* Build the algorithm with the stochastic gradient descent strategy and the mini-batch gradient descent strategy, and simply compare the performance.\n",
        "\n",
        "* Increase the number of the nodes in the hidden layers to see whether it can improve the performance of the algorithm.\n",
        "\n",
        "* Add ReLU as another activation function, and compare the performance of the algorithms with different activation functions.\n",
        "\n",
        "\n",
        "### Tools\n",
        "\n",
        "In order to do this, I need the following libraries:\n",
        "\n",
        "* [matplotlib](http://metplotlib.org)\n",
        "* [numpy](https://numpy.org/doc/stable/index.html)\n",
        "* [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "To load the Fashion MNIST data, we also need to import [keras.dataset](https://keras.io/api/datasets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yZyJw-jGxbAQ"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBriGPc3xbAR"
      },
      "source": [
        "### Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7x2Uf0-xbAS",
        "outputId": "0c1e5d7b-e779-450c-bb1f-d433ff5edc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the Fashion MNIST data\n",
        "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6PSlrRfxbAS",
        "outputId": "811ad4cf-0ed7-473b-933f-2d314f23115c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# shape of the training set\n",
        "train_X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxVRkc1fxbAS",
        "outputId": "b82939a0-2436-4ed9-c35b-62322bcadebb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# shape of the first matrix in the training set\n",
        "train_X[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWz1M9wUxbAT",
        "outputId": "6f3b80c3-85ac-4729-93bd-55777c0abfc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# shape of the test set\n",
        "test_X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hkfNd5OxbAT"
      },
      "source": [
        "The training set has $60000$ pictures with $28 \\times 28$ pixel. The test set has $10000$ pictures with the same $28 \\times 28$ pixel.\n",
        "\n",
        "First data point in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "VNcpTpZkxbAT",
        "outputId": "f2741452-a4cf-4816-c778-fbbe0a58f199"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x785d336b9e40>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show the first matrix in the training set\n",
        "plt.imshow(train_X[0], cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW8-o3BvxbAU",
        "outputId": "c2848c72-b072-4163-df8e-6e162c96fba6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# first label of the training set\n",
        "train_y[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0cZOH-WxbAU"
      },
      "source": [
        "The matrix and the label match with true label (9: Ankle boot).\n",
        "\n",
        "\n",
        "Evaluates the range of the grey scale ( ùë• ) in the training matrics. If the range is big, it may require scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S9FP1ULuxbAV",
        "outputId": "6ed03495-5955-45c6-bea1-b5164d0d7657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "np.max(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9yrXc06xbAW",
        "outputId": "0d1ab1db-60b7-4c1d-9012-229e50d91990"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "np.min(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K40vbNYfxbAX",
        "outputId": "0d548b6d-4e88-475a-e30e-2c5f04941c06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "np.max(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTgBidi4xbAX",
        "outputId": "d66cf3db-b7ee-43db-d646-1ebd171605f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "np.min(test_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lankq7tMxbAY"
      },
      "source": [
        "The the range of the grey scale ($x$) is $(0, 255)$, and thus, it requires scaling. To scale it divide it by the maximun value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XL-coY5qxbAY"
      },
      "outputs": [],
      "source": [
        "# Scale down X\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Px0ALXnxbAY"
      },
      "source": [
        "Thereafter, reshape the input matrics ($X$) and output matrics ($y$) to a desire pattern that can fit our algorithm.\n",
        "\n",
        "First, for the input matrics ($X$), we need to flatten the $28 \\times 28$ matrix, and reshape it to a $784 \\times 1$ vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VWQtoX-VxbAY"
      },
      "outputs": [],
      "source": [
        "# X will temp store flattened matrices\n",
        "X = []\n",
        "for x in train_X:\n",
        "  X.append(x.flatten().reshape(784, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx32HkhXxbAY"
      },
      "source": [
        "Second, for the output matrics ($y$), it is a single number, and we need to do the One Hot Encoding and represent it using a $10 \\times 1$ vector. The idea is just the same as we did for the digits. For example,\n",
        "\n",
        "$$y=9 \\overset{\\text {One Hot encode}}{\\rightarrow} y =\\begin{bmatrix}\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "1\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qBSU2i3qxbAZ"
      },
      "outputs": [],
      "source": [
        "# Y will temp store one-hot encoded label vectors\n",
        "Y = []\n",
        "for y in train_y:\n",
        "  temp_vec = np.zeros((10, 1))\n",
        "  temp_vec[y][0] = 1.0\n",
        "  Y.append(temp_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uafQRW6WxbAZ"
      },
      "outputs": [],
      "source": [
        "# Our data will be stored as a list of tuples\n",
        "train_data = [p for p in zip(X, Y)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jo0bZFrxbAZ"
      },
      "source": [
        "So far, the \"train_data\" would have one column with the flattened $X$ vectors and the one column with the One Hot encoded label vectors. For better understanding of data pick the first data point and print the One Hot encoded label and the original true value to check the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlgz-5YVxbAZ",
        "outputId": "7d991c7b-35e1-460f-f767-fd1ef2a768eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "p = train_data[0]\n",
        "print(p[1])\n",
        "print(train_y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eSIGRQhxbAZ"
      },
      "source": [
        "The transformation is correct! Repeat the same for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lz3rLs5RxbAa"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "for x in test_X:\n",
        "  X.append(x.flatten().reshape(784, 1))\n",
        "\n",
        "Y = []\n",
        "for y in test_y:\n",
        "  temp_vec = np.zeros((10, 1))\n",
        "  temp_vec[y][0] = 1.0\n",
        "  Y.append(temp_vec)\n",
        "\n",
        "test_data = [p for p in zip(X, Y)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Nyh6wJxbAa"
      },
      "source": [
        "---\n",
        "\n",
        "### Build the Algorithm\n",
        "\n",
        "To build the algorithm, there are several functions need to be defined.\n",
        "\n",
        "#### Activation Function\n",
        "Use  sigmoid function as the activation function,\n",
        "$$\\sigma(z)=\\frac {1}{1+e^{-z}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hS5CHMbAxbAa"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z)*(1.0-sigmoid(z))#### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts3UJ4BWxbAb"
      },
      "source": [
        "#### Loss Function\n",
        "we use the Mean Sqaure Error cost:\n",
        "$$\n",
        "C = C(W, b) = \\frac{1}{2}\\sum_{i=1}^n(a^i - y^i)^2.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7mXDcYMyxbAb"
      },
      "outputs": [],
      "source": [
        "def mse(a, y):\n",
        "  return .5*sum((a[i]-y[i])**2 for i in range(10))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X78sClwJxbAb"
      },
      "source": [
        "#### The Function to Initialize the Weights and Bias\n",
        "\n",
        "For algorithm implementations fix the number of nodes in the four layers to be $748$, $60$, $60$, and $10$, repectively. And thus, the dimemsions of the weights are ($60$, $784$), ($60$, $60$), ($10$, $60$), repectively, and the dimensions of the bias are ($60$, $1$), ($60$, $1$), ($10$, $1$), repectively.\n",
        "\n",
        "To increase efficiency use scaling initializer while intializing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "c7iB08AIrCbc"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(layers = [784, 60, 60, 10]):\n",
        "  W = [[0.0]]  #add weight_0, to match the shape\n",
        "  B = [[0.0]]\n",
        "  for i in range(1, len(layers)):\n",
        "    w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])  #Scaling initializer to scale the shape\n",
        "    b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
        "\n",
        "    W.append(w_temp)\n",
        "    B.append(b_temp)\n",
        "  return W, B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_AGxegAxbAc"
      },
      "source": [
        "Let's check!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2NrwX_wRrEx8"
      },
      "outputs": [],
      "source": [
        "W, B = initialize_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m1Xn3ZXJrN57"
      },
      "outputs": [],
      "source": [
        "x, y = train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJccNp4ZxbAd"
      },
      "source": [
        "First layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "G1cBt3SotSqZ"
      },
      "outputs": [],
      "source": [
        "a0 = x\n",
        "z1 = (W[1] @ a0) + B[1]\n",
        "a1 = sigmoid(z1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dflUcRI-BpL",
        "outputId": "0e06b12f-2665-40ef-f398-1bd21f3dea1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "a1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfCnglkFxbAd"
      },
      "source": [
        "The shape of the output of the first layer match the desire dimension.\n",
        "\n",
        "Second layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89DswyFrtiT5",
        "outputId": "670ec9a0-7380-4df3-c352-904c4f31e80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 1)\n"
          ]
        }
      ],
      "source": [
        "z2 = (W[2] @ a1) + B[2]\n",
        "a2 = sigmoid(z2)\n",
        "print(a2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV5jTOjIxbAd"
      },
      "source": [
        "The shape of the output of the second layer match the desire dimension.\n",
        "\n",
        "And the third layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uB0Mk4otizq",
        "outputId": "933f63ee-d74f-4489-d4d0-a010366662d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "z3 = (W[3] @ a2) + B[3]\n",
        "a3 = sigmoid(z3)\n",
        "print(a3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0JZ6PjmxbAe"
      },
      "source": [
        "The shape of the output of the thired layer match the desire dimension.\n",
        "\n",
        "These results suggest that the \"innitialize_weights\" function works well. And then, we can put the input and output of the layers together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HdrA_7UOt5DX"
      },
      "outputs": [],
      "source": [
        "W, B = initialize_weights(layers=[784, 60, 60, 10])\n",
        "x, y = train_data[0]\n",
        "Z = [[0.0]]\n",
        "A = [x]\n",
        "L = len(B)\n",
        "for i in range(1, L):\n",
        "  z = (W[i] @ A[i-1]) + B[i]\n",
        "  a = sigmoid(z)\n",
        "\n",
        "  Z.append(z)\n",
        "  A.append(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB_O0_zWxbAe"
      },
      "source": [
        "To check whether the loop works well, we can print the shape of the output of the layer before the last layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb0DO1EhurnG",
        "outputId": "8d021d35-695e-496f-c1d5-0d67a6053e5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "A[-1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFxM1ocxbAe"
      },
      "source": [
        "The shape is correct!\n",
        "\n",
        "#### Output Error\n",
        "\n",
        "The output error is\n",
        "\n",
        "$$\\delta^{l-1}=\\triangledown_{a^{l-1}}C \\otimes \\sigma'(z^{l-1})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3jAcb248uswW"
      },
      "outputs": [],
      "source": [
        "# Measure the output error\n",
        "deltas = dict()\n",
        "delta_last = (A[-1] - y)*sigmoid_prime(Z[-1])\n",
        "deltas[L-1] = delta_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ7zYBAHBWDd",
        "outputId": "d032b640-36c3-480b-c550-c0022acb53da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.14788663],\n",
              "       [ 0.08032605],\n",
              "       [ 0.14778251],\n",
              "       [ 0.14512939],\n",
              "       [ 0.11334653],\n",
              "       [ 0.08169374],\n",
              "       [ 0.14813583],\n",
              "       [ 0.13996792],\n",
              "       [ 0.09375031],\n",
              "       [-0.14806801]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "deltas[L-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU37a-0UxbAf"
      },
      "source": [
        "#### Neuron Error\n",
        "\n",
        "According to the output error, for $l=L-2,...,1$, the neuron error is\n",
        "\n",
        "$$\\delta^{l}=\\left ( (w^{l+1})^T a^{l+1} \\right ) \\otimes \\sigma'(z^{l})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3M5rIEebvg90"
      },
      "outputs": [],
      "source": [
        "# calculate the neuron error\n",
        "for l in range(L-2, 0, -1):\n",
        "  deltas[l] = (W[l+1].T @ deltas[l+1])*sigmoid_prime(Z[l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms6IMLOIvhqW",
        "outputId": "cdda0f22-5015-4194-8472-b3095795edec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "deltas[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfWDQW3sxbAf",
        "outputId": "61d7d6fe-3701-453c-c1d8-f046e08cbb78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "deltas[2].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REsW1lTxxbAf",
        "outputId": "e657377b-fb7a-41ff-b955-df8cd2ad7902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "deltas[3].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvbT99ubxbAg"
      },
      "source": [
        "The dimensions of the outputs of the layers are all correctly, suggesting that the codes for the output error and the neuron error work correctly.\n",
        "\n",
        "The gradient descent will be used to optimize the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wDGqFoFQwTqE"
      },
      "outputs": [],
      "source": [
        "# Set the learning rate\n",
        "alpha = 0.04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HKCHpDeVwgtj"
      },
      "outputs": [],
      "source": [
        "# To update the weights and bias\n",
        "for i in range(1, 4):\n",
        "  W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
        "  B[i] = B[i] - alpha*deltas[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMMZfrNzxbAg"
      },
      "source": [
        "#### Feedforward Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "K8mHDPhiwxC0"
      },
      "outputs": [],
      "source": [
        "def forward_pass(W, B, p, predict_vector = False):\n",
        "  Z =[[0.0]]\n",
        "  A = [p[0]]\n",
        "  L = len(W)\n",
        "  for i in range(1, L):\n",
        "    z = (W[i] @ A[i-1]) + B[i]\n",
        "    a = sigmoid(z)\n",
        "\n",
        "    Z.append(z)\n",
        "    A.append(a)\n",
        "\n",
        "  if predict_vector == True:\n",
        "    return A[-1]\n",
        "  else:\n",
        "    return Z, A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "877pRiwRxbAg"
      },
      "source": [
        "#### Store the Neuron Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7N5Q8YXvxbAg"
      },
      "outputs": [],
      "source": [
        "def deltas_dict(W, B, p):\n",
        "  Z, A = forward_pass(W, B, p)\n",
        "  L = len(W)\n",
        "  deltas = dict()\n",
        "  deltas[L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
        "  for l in range(L-2, 0, -1):\n",
        "    deltas[l] = (W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
        "\n",
        "  return A, deltas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J8Ci5GLxbAg"
      },
      "source": [
        "#### Average of Mean Squared Error\n",
        "\n",
        "This is to evaluate the overall performance for each step (epoch/mini-batch depends on settings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nayiEoIVyepQ"
      },
      "outputs": [],
      "source": [
        "def MSE(W, B, data):\n",
        "  c = 0.0\n",
        "  for p in data:\n",
        "    a = forward_pass(W, B, p, predict_vector=True)\n",
        "    c += mse(a, p[1])\n",
        "  return c/len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwQcmuiKxbAh"
      },
      "source": [
        "---\n",
        "\n",
        "### Implement the Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7uMRvUzO3w",
        "outputId": "aacf587b-1326-4165-a247-558a46d61d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 1.5515279221127642\n"
          ]
        }
      ],
      "source": [
        "# Calculate the initial cost\n",
        "W, B = initialize_weights()\n",
        "print(f\"Initial Cost = {MSE(W, B, train_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "jioHLhRrzcow",
        "outputId": "12991e7b-149c-4974-c14c-f907b0b5306a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Value = 6\n",
            "Actual Value = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Show the performance\n",
        "i = np.random.randint(0, len(test_X))\n",
        "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
        "print(f\"Predicted Value = {prediction}\")\n",
        "print(f\"Actual Value = {test_y[i]}\")\n",
        "plt.imshow(test_X[i], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4tdzXrqxbAh"
      },
      "source": [
        "The prediction is correct.\n",
        "\n",
        "Next, we train the model using Stochasitc Gradient Descent strategy.\n",
        "\n",
        "#### Stochasitc Gradient Descent\n",
        "\n",
        "First, we define a SGD function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F-Ggl8gj0VW-"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(W, B, data, alpha = 0.04, epochs = 3):\n",
        "  L = len(W)\n",
        "  print(f\"Initial Cost = {MSE(W, B, data)}\")\n",
        "  for k in range(epochs):\n",
        "    for p in data:\n",
        "      A, deltas = deltas_dict(W, B, p)\n",
        "      for i in range(1, L):\n",
        "        W[i] = W[i] - alpha*deltas[i]@A[i-1].T\n",
        "        B[i] = B[i] - alpha*deltas[i]\n",
        "    print(f\"{k} Cost = {MSE(W, B, data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LYBos_QxbAi"
      },
      "source": [
        "Let's train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ5TxxO1xbAi",
        "outputId": "ab9228ee-94c6-44b5-fc78-e26fe179c7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 1.5515279221127642\n",
            "0 Cost = 0.12391589943967744\n",
            "1 Cost = 0.10837236308256198\n",
            "2 Cost = 0.10053790724803204\n"
          ]
        }
      ],
      "source": [
        "stochastic_gradient_descent(W, B, train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7C7x3l0xbAi"
      },
      "source": [
        "The costs of every epoch gradually decrease. The training process works well.\n",
        "\n",
        "We can also take a look at one example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "ElZ7u0KXxbAi",
        "outputId": "ea0e978f-2daf-46f1-8522-d83e56c55ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Value = 5\n",
            "Actual Value = 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "i = np.random.randint(0, len(test_X))\n",
        "prediction = np.argmax(forward_pass(W, B, test_data[i], predict_vector=True))\n",
        "print(f\"Predicted Value = {prediction}\")\n",
        "print(f\"Actual Value = {test_y[i]}\")\n",
        "plt.imshow(test_X[i], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaaF0ptfxbAi"
      },
      "source": [
        "The prediction is correct!\n",
        "\n",
        "We can put all the functions together to make a class. In the meantime, we also define another strategy, **mini-batch gradient descent** to optimize the algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WCsZvETL0fk-"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron():\n",
        "\n",
        "  def __init__(self, layers = [784, 60, 60, 10]):\n",
        "    self.layers = layers\n",
        "    self.L = len(self.layers)\n",
        "    self.W =[[0.0]]\n",
        "    self.B = [[0.0]]\n",
        "    for i in range(1, self.L):\n",
        "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
        "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
        "\n",
        "      self.W.append(w_temp)\n",
        "      self.B.append(b_temp)\n",
        "\n",
        "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
        "    self.layers = layers\n",
        "    self.L = len(self.layers)\n",
        "    self.W = [[0.0]]\n",
        "    self.B = [[0.0]]\n",
        "    for i in range(1, self.L):\n",
        "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
        "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
        "\n",
        "      self.W.append(w_temp)\n",
        "      self.B.append(b_temp)\n",
        "\n",
        "\n",
        "  def forward_pass(self, p, predict_vector = False):\n",
        "    Z =[[0.0]]\n",
        "    A = [p[0]]\n",
        "    for i in range(1, self.L):\n",
        "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
        "      a = sigmoid(z)\n",
        "      Z.append(z)\n",
        "      A.append(a)\n",
        "\n",
        "    if predict_vector == True:\n",
        "      return A[-1]\n",
        "    else:\n",
        "      return Z, A\n",
        "\n",
        "  def MSE(self, data):\n",
        "    c = 0.0\n",
        "    for p in data:\n",
        "      a = self.forward_pass(p, predict_vector=True)\n",
        "      c += mse(a, p[1])\n",
        "    return c/len(data)\n",
        "\n",
        "  def deltas_dict(self, p):\n",
        "    Z, A = self.forward_pass(p)\n",
        "    deltas = dict()\n",
        "    deltas[self.L-1] = (A[-1] - p[1])*sigmoid_prime(Z[-1])\n",
        "    for l in range(self.L-2, 0, -1):\n",
        "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) * sigmoid_prime(Z[l])\n",
        "\n",
        "    return A, deltas\n",
        "\n",
        "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
        "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
        "    for k in range(epochs):\n",
        "      for p in data:\n",
        "        A, deltas = self.deltas_dict(p)\n",
        "        for i in range(1, self.L):\n",
        "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
        "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
        "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
        "\n",
        "\n",
        "  def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
        "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
        "    data_length = len(data)\n",
        "    for k in range(epochs):\n",
        "        for j in range(0, data_length-batch_size, batch_size):\n",
        "            delta_list = []\n",
        "            A_list = []\n",
        "            for p in data[j:j+batch_size]:\n",
        "                A, deltas = self.deltas_dict(p)\n",
        "                delta_list.append(deltas)\n",
        "                A_list.append(A)\n",
        "\n",
        "                for i in range(1, self.L):\n",
        "                    self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
        "                    self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
        "    print(f\"{k} Cost = {self.MSE(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrerLWUxbAj"
      },
      "source": [
        "Let's try our class!\n",
        "\n",
        "We can also setup 60 nodes for both hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JHxk-iGLHzEG"
      },
      "outputs": [],
      "source": [
        "net = MultilayerPerceptron(layers=[784, 60, 60, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usBgRh3_xbAj"
      },
      "source": [
        "To use the stochastic gradient descent strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpgtRzZ3xbAj",
        "outputId": "757051ce-7231-4465-bd53-02353fc7a71b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 1.6260190429344097\n",
            "2 Cost = 0.09920177997904978\n"
          ]
        }
      ],
      "source": [
        "net.stochastic_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbba7LKAxbAj"
      },
      "source": [
        "To use the mini-batch gradient descent strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnGcnC9zxbAj",
        "outputId": "2cbdc233-9018-4c30-a856-fd5e8fe733aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 0.09920177997904978\n",
            "2 Cost = 0.09116696970818902\n"
          ]
        }
      ],
      "source": [
        "net.mini_batch_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNf6uopnxbAj"
      },
      "source": [
        "Compare these two strategy, the mini-batch gradient descent has a smaller cost than the stochastic gradient descent.\n",
        "\n",
        "Notably, stochastic gradient descent is more efficient than the min-batch gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKqKcADdxbAk"
      },
      "source": [
        "---\n",
        "\n",
        "### Increase the Number of Nodes in the Hidden Layers\n",
        "\n",
        "Fix 100 nodes for both hidden layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UgPFmJXJxbAk"
      },
      "outputs": [],
      "source": [
        "net = MultilayerPerceptron(layers=[784, 100, 100, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPrWFthOxbAk"
      },
      "source": [
        "To use the stochastic gradient descent strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wSPs133H_HT",
        "outputId": "d0b1b614-f9c1-4b52-8f78-894894fcd729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 1.5638814729751542\n",
            "2 Cost = 0.10096310103725563\n"
          ]
        }
      ],
      "source": [
        "net.stochastic_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izl9LTdKxbAk"
      },
      "source": [
        "To use the mini-batch gradient descent strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJVa0rfEIBXj",
        "outputId": "87af02e0-4626-42f4-999a-1bbed2723d00"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Cost = 0.10096310103725563\n",
            "2 Cost = 0.09125065442912618\n"
          ]
        }
      ],
      "source": [
        "net.mini_batch_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYceCIu-xbAk"
      },
      "source": [
        "The mini-batch gradient descent still has a smaller cost than the stochastic gradient descent.\n",
        "\n",
        "Increasing the number of nodes in the hidden layers does not greatly change the loss in this case.\n",
        "\n",
        "---\n",
        "\n",
        "### Add ReLU as Another Activation Function\n",
        "\n",
        "The ReLU function is\n",
        "$$R(z)= max(0,z)$$\n",
        "\n",
        "The derivation of the ReLU function is\n",
        "\n",
        "$$R'(z)= \\left\\{\\begin{matrix}\n",
        "0, \\text{ if }z < 0\\\\\n",
        "1, \\text{ if }z > 0\n",
        "\\end{matrix}\\right.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2cOuodcixbAl"
      },
      "outputs": [],
      "source": [
        "def ReLU(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def ReLU_prime(z):\n",
        "    if z > 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acp6OxxbxbAl",
        "outputId": "c247e70c-b414-4e05-df82-42dfda06d886"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ReLU(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvhLlFaLxbAl",
        "outputId": "2e7d1893-4d05-483d-cdc0-7cf098727e60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ReLU_prime(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k1pc2cZxbAl"
      },
      "source": [
        "Then we can add the ReLU function and modify the \"MultilayerPerceptron\" class:\n",
        "* Add another parameter, \"activation_type\"\n",
        "* Define \"activation\" function, which returns the value of the specified activation function\n",
        "* Define \"activation_prime\" function, which returns the value of the derivation of the specified activation function\n",
        "* Modified other function to call the \"activation\" and \"activation_prime\" functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "al-pchjTxbAl"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron():\n",
        "\n",
        "  def __init__(self, layers = [784, 60, 60, 10], activation_type = 'sigmoid'):\n",
        "    self.layers = layers\n",
        "    self.activation_type = activation_type\n",
        "    self.L = len(self.layers)\n",
        "    self.W =[[0.0]]\n",
        "    self.B = [[0.0]]\n",
        "    for i in range(1, self.L):\n",
        "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
        "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
        "\n",
        "      self.W.append(w_temp)\n",
        "      self.B.append(b_temp)\n",
        "\n",
        "  def reset_weights(self, layers = [784, 60, 60, 10]):\n",
        "    self.layers = layers\n",
        "    self.L = len(self.layers)\n",
        "    self.W = [[0.0]]\n",
        "    self.B = [[0.0]]\n",
        "    for i in range(1, self.L):\n",
        "      w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
        "      b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
        "\n",
        "      self.W.append(w_temp)\n",
        "      self.B.append(b_temp)\n",
        "\n",
        "  def activation(self, z, type):\n",
        "    if type == \"sigmoid\":\n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "    elif type == \"ReLU\":\n",
        "        return np.maximum(0, z)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "  def activation_prime(self, z, type):\n",
        "    if type == \"sigmoid\":\n",
        "        return sigmoid(z)*(1.0-sigmoid(z))\n",
        "    elif type == \"ReLU\":\n",
        "        return np.where(z > 0, 1.0, 0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "  def forward_pass(self, p, predict_vector = False):\n",
        "    Z =[[0.0]]\n",
        "    A = [p[0]]\n",
        "    for i in range(1, self.L):\n",
        "      z = (self.W[i] @ A[i-1]) + self.B[i]\n",
        "      a = self.activation(z, self.activation_type)\n",
        "      Z.append(z)\n",
        "      A.append(a)\n",
        "\n",
        "    if predict_vector == True:\n",
        "      return A[-1]\n",
        "    else:\n",
        "      return Z, A\n",
        "\n",
        "  def MSE(self, data):\n",
        "    c = 0.0\n",
        "    for p in data:\n",
        "      a = self.forward_pass(p, predict_vector=True)\n",
        "      c += mse(a, p[1])\n",
        "    return c/len(data)\n",
        "\n",
        "  def deltas_dict(self, p):\n",
        "    Z, A = self.forward_pass(p)\n",
        "    deltas = dict()\n",
        "    deltas[self.L-1] = (A[-1] - p[1])*self.activation_prime(Z[-1], self.activation_type)\n",
        "    for l in range(self.L-2, 0, -1):\n",
        "      deltas[l] = (self.W[l+1].T @ deltas[l+1]) * self.activation_prime(Z[l], self.activation_type)\n",
        "\n",
        "    return A, deltas\n",
        "\n",
        "  def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
        "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
        "    for k in range(epochs):\n",
        "      for p in data:\n",
        "        A, deltas = self.deltas_dict(p)\n",
        "        for i in range(1, self.L):\n",
        "          self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
        "          self.B[i] = self.B[i] - alpha*deltas[i]\n",
        "    print(f\"{k} Cost = {self.MSE(data)}\")\n",
        "\n",
        "\n",
        "  def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
        "    print(f\"Initial Cost = {self.MSE(data)}\")\n",
        "    data_length = len(data)\n",
        "    for k in range(epochs):\n",
        "        for j in range(0, data_length-batch_size, batch_size):\n",
        "            delta_list = []\n",
        "            A_list = []\n",
        "            for p in data[j:j+batch_size]:\n",
        "                A, deltas = self.deltas_dict(p)\n",
        "                delta_list.append(deltas)\n",
        "                A_list.append(A)\n",
        "\n",
        "                for i in range(1, self.L):\n",
        "                    self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
        "                    self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
        "    print(f\"{k} Cost = {self.MSE(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_q65p-LxbAm"
      },
      "source": [
        "To use ReLU as the activation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "K8r5mEuRxbAm"
      },
      "outputs": [],
      "source": [
        "net = MultilayerPerceptron(layers=[784, 60, 60, 10], activation_type = 'ReLU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ou0ZBuMxbAm",
        "outputId": "0c2ab77c-ac8f-453f-d1f4-4e7ce5862f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 0.7678666982884504\n",
            "2 Cost = 0.5\n"
          ]
        }
      ],
      "source": [
        "net.stochastic_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGf0f2Y4xbAm",
        "outputId": "bbade272-c77c-459e-cacd-2fd3898dbe19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost = 0.5\n",
            "2 Cost = 0.5\n"
          ]
        }
      ],
      "source": [
        "net.mini_batch_gradient_descent(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3HDqJPaxbAm"
      },
      "source": [
        "When using ReLU as the activation function, the mini-batch gradient descent also has a smaller cost than the stochastic gradient descent.\n",
        "\n",
        "Compare the results of the sigmoid function, ReLU gains a larger, suggesting that the sigmoid function is better than ReLU in this case.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The algorithm performs well.\n",
        "\n",
        "To classify the fashion categories, the mini-batch gradient descent strategy performs better than the stochastic gradient descent strategy.\n",
        "\n",
        "Increaing the number of nodes in the hidden layer does not affect the performance of the algorithm in this case.\n",
        "\n",
        "In the meantime, sigmoid function is the better than ReLU to classify the fashion categories."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "677d7cb3073a0b8867375a4e72362c1c4e2840c39316c25dc6682a3b67a3dfcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}